{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d95196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanuja/Desktop/preference-tuned-summarizer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a8c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61697597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and base model (DistilGPT2 is small and fast)\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7dac450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8b4dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tokenizer handles padding\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adcf89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DPO dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/dpo_format.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a322e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments (use DPOConfig instead!)\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"../models/distilgpt2-dpo-checkpoint\",\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=False,   # Disable bfloat16 (MPS usually does NOT support this fully yet)\n",
    "    fp16=False,   # Disable fp16 (not supported on MPS)\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    padding_value=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2988794",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize DPOTrainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainer = \u001b[43mDPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/preference-tuned-summarizer/.venv/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:452\u001b[39m, in \u001b[36mDPOTrainer.__init__\u001b[39m\u001b[34m(self, model, ref_model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config)\u001b[39m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    450\u001b[39m         eval_dataset = \u001b[38;5;28mself\u001b[39m._prepare_dataset(eval_dataset, processing_class, args, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# self.model_accepts_loss_kwargs to False to enable scaling.\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;28mself\u001b[39m.model_accepts_loss_kwargs = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/preference-tuned-summarizer/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/preference-tuned-summarizer/.venv/lib/python3.12/site-packages/transformers/trainer.py:465\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28mself\u001b[39m.is_in_train = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28mself\u001b[39m._memory_tracker = TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m.args.skip_memory_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/preference-tuned-summarizer/.venv/lib/python3.12/site-packages/transformers/trainer.py:5208\u001b[39m, in \u001b[36mTrainer.create_accelerator_and_postprocess\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5205\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequires accelerate>1.3.0 to use Tensor Parallelism.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5207\u001b[39m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5208\u001b[39m \u001b[38;5;28mself\u001b[39m.accelerator = \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5209\u001b[39m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[32m   5210\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather_for_metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/preference-tuned-summarizer/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:547\u001b[39m, in \u001b[36mAccelerator.__init__\u001b[39m\u001b[34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m.native_amp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m    538\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    539\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msdaa\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    546\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(check_is_tpu=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device.type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    548\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m.scaler_handler.to_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# FSDP2 doesn't use ShardedGradScaler, don't want to modify `get_grad_scaler`, rather create a simple utility\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "# Initialize DPOTrainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6143c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d277e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model(\"../models/distilgpt2-dpo-checkpoint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
